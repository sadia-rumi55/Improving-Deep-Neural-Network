{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_tutorial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN0ImOhzuum6uyGYuiacu7o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sadia-rumi55/Improving-Deep-Neural-Network/blob/master/tensorflow_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFl_mlOf57AE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import ops\n",
        "from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
        "\n",
        "%matplotlib inline\n",
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkIgI9YB6CQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat = tf.constant(36, name='y_hat')            # Define y_hat constant. Set to 36.\n",
        "y = tf.constant(39, name='y')                    # Define y. Set to 39\n",
        "\n",
        "loss = tf.Variable((y - y_hat)**2, name='loss')  # Create a variable for the loss\n",
        "\n",
        "init = tf.global_variables_initializer()         # When init is run later (session.run(init)),\n",
        "                                                 # the loss variable will be initialized and ready to be computed\n",
        "with tf.Session() as session:                    # Create a session and print the output\n",
        "    session.run(init)                            # Initializes the variables\n",
        "    print(session.run(loss))                     # Prints the loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvjRZ-zY6Cjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = tf.constant(2)\n",
        "b = tf.constant(10)\n",
        "c = tf.multiply(a,b)\n",
        "print(c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Znz0vPEO6Coj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()\n",
        "print(sess.run(c))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX1rvJEa6Chd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change the value of x in the feed_dict\n",
        "\n",
        "x = tf.placeholder(tf.int64, name = 'x')\n",
        "print(sess.run(2 * x, feed_dict = {x: 3}))\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs4bZNMg6Cfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: linear_function\n",
        "\n",
        "def linear_function():\n",
        "    \"\"\"\n",
        "    Implements a linear function: \n",
        "            Initializes W to be a random tensor of shape (4,3)\n",
        "            Initializes X to be a random tensor of shape (3,1)\n",
        "            Initializes b to be a random tensor of shape (4,1)\n",
        "    Returns: \n",
        "    result -- runs the session for Y = WX + b \n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    \n",
        "    ### START CODE HERE ### (4 lines of code)\n",
        "    X = np.random.randn(3, 1)\n",
        "    W = np.random.randn(4, 3)\n",
        "    b = np.random.randn(4, 1)\n",
        "    Y = tf.add(tf.matmul(W, X), b)\n",
        "    ### END CODE HERE ### \n",
        "    \n",
        "    # Create the session using tf.Session() and run it with sess.run(...) on the variable you want to calculate\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    sess = tf.Session()\n",
        "    result = sess.run(Y)\n",
        "    ### END CODE HERE ### \n",
        "    \n",
        "    # close the session \n",
        "    sess.close()\n",
        "\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omzbrn8f6Cd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print( \"result = \" + str(linear_function()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_kWE5c86CcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: sigmoid\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Computes the sigmoid of z\n",
        "    \n",
        "    Arguments:\n",
        "    z -- input value, scalar or vector\n",
        "    \n",
        "    Returns: \n",
        "    results -- the sigmoid of z\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### ( approx. 4 lines of code)\n",
        "    # Create a placeholder for x. Name it 'x'.\n",
        "    x = tf.placeholder(tf.float32, name=\"x\")\n",
        "\n",
        "    # compute sigmoid(x)\n",
        "    sigmoid = tf.sigmoid(x)\n",
        "\n",
        "    # Create a session, and run it. Please use the method 2 explained above. \n",
        "    # You should use a feed_dict to pass z's value to x. \n",
        "    with tf.Session() as sess: \n",
        "        # Run session and call the output \"result\"\n",
        "        result = result = sess.run(sigmoid, feed_dict = {x: z})\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR9UBuMJ6Caa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (\"sigmoid(0) = \" + str(sigmoid(0)))\n",
        "print (\"sigmoid(12) = \" + str(sigmoid(12)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko5PYduf6CYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: cost\n",
        "\n",
        "def cost(logits, labels):\n",
        "    \"\"\"\n",
        "    Computes the cost using the sigmoid cross entropy\n",
        "    \n",
        "    Arguments:\n",
        "    logits -- vector containing z, output of the last linear unit (before the final sigmoid activation)\n",
        "    labels -- vector of labels y (1 or 0) \n",
        "    \n",
        "    Note: What we've been calling \"z\" and \"y\" in this class are respectively called \"logits\" and \"labels\" \n",
        "    in the TensorFlow documentation. So logits will feed into z, and labels into y. \n",
        "    \n",
        "    Returns:\n",
        "    cost -- runs the session of the cost (formula (2))\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### \n",
        "    \n",
        "    # Create the placeholders for \"logits\" (z) and \"labels\" (y) (approx. 2 lines)\n",
        "    z = tf.placeholder(tf.float32, name=\"z\")\n",
        "    y = tf.placeholder(tf.float32, name=\"y\")\n",
        "    \n",
        "    # Use the loss function (approx. 1 line)\n",
        "    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=z, labels=y)\n",
        "    \n",
        "    # Create a session (approx. 1 line). See method 1 above.\n",
        "    sess = tf.Session()\n",
        "    \n",
        "    # Run the session (approx. 1 line).\n",
        "    cost = sess.run(cost, feed_dict={z: logits, y: labels})\n",
        "    \n",
        "    # Close the session (approx. 1 line). See method 1 above.\n",
        "    sess.close()\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYvTyluY6CWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logits = sigmoid(np.array([0.2, 0.4, 0.7, 0.9]))\n",
        "cost = cost(logits, np.array([0, 0, 1, 1]))\n",
        "print (\"cost = \" + str(cost))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCIw4vEf6CUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: one_hot_matrix\n",
        "\n",
        "def one_hot_matrix(labels, C):\n",
        "    \"\"\"\n",
        "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
        "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
        "                     will be 1. \n",
        "                     \n",
        "    Arguments:\n",
        "    labels -- vector containing the labels \n",
        "    C -- number of classes, the depth of the one hot dimension\n",
        "    \n",
        "    Returns: \n",
        "    one_hot -- one hot matrix\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)\n",
        "    C = tf.constant(C, name='C')\n",
        "    \n",
        "    # Use tf.one_hot, be careful with the axis (approx. 1 line)\n",
        "    one_hot_matrix = tf.one_hot(indices=labels, depth=C, axis=0)\n",
        "    \n",
        "    # Create the session (approx. 1 line)\n",
        "    sess = tf.Session()\n",
        "    \n",
        "    # Run the session (approx. 1 line)\n",
        "    one_hot = sess.run(one_hot_matrix)\n",
        "    \n",
        "    # Close the session (approx. 1 line). See method 1 above.\n",
        "    sess.close()\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELsHnpaW6CNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: ones\n",
        "\n",
        "def ones(shape):\n",
        "    \"\"\"\n",
        "    Creates an array of ones of dimension shape\n",
        "    \n",
        "    Arguments:\n",
        "    shape -- shape of the array you want to create\n",
        "        \n",
        "    Returns: \n",
        "    ones -- array containing only ones\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # Create \"ones\" tensor using tf.ones(...). (approx. 1 line)\n",
        "    ones = tf.ones(shape)\n",
        "    \n",
        "    # Create the session (approx. 1 line)\n",
        "    sess = tf.Session()\n",
        "    \n",
        "    # Run the session to compute 'ones' (approx. 1 line)\n",
        "    ones = sess.run(ones)\n",
        "    \n",
        "    # Close the session (approx. 1 line). See method 1 above.\n",
        "    sess.close()\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    return ones"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tt2o3qR7Gp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (\"ones = \" + str(ones([3])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-ZHHmPh7Gtc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the dataset\n",
        "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv_KmSlR7v_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example of a picture\n",
        "index = 0\n",
        "plt.imshow(X_train_orig[index])\n",
        "print (\"y = \" + str(np.squeeze(Y_train_orig[:, index])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SfwtVTb7wP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Flatten the training and test images\n",
        "X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\n",
        "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n",
        "# Normalize image vectors\n",
        "X_train = X_train_flatten / 255.\n",
        "X_test = X_test_flatten / 255.\n",
        "# Convert training and test labels to one hot matrices\n",
        "Y_train = convert_to_one_hot(Y_train_orig, 6)\n",
        "Y_test = convert_to_one_hot(Y_test_orig, 6)\n",
        "\n",
        "print(\"number of training examples = \" + str(X_train.shape[1]))\n",
        "print(\"number of test examples = \" + str(X_test.shape[1]))\n",
        "print(\"X_train shape: \" + str(X_train.shape))\n",
        "print(\"Y_train shape: \" + str(Y_train.shape))\n",
        "print(\"X_test shape: \" + str(X_test.shape))\n",
        "print(\"Y_test shape: \" + str(Y_test.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27TNdY8p7wUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: create_placeholders\n",
        "\n",
        "def create_placeholders(n_x, n_y):\n",
        "    \"\"\"\n",
        "    Creates the placeholders for the tensorflow session.\n",
        "    \n",
        "    Arguments:\n",
        "    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n",
        "    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n",
        "    \n",
        "    Returns:\n",
        "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
        "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
        "    \n",
        "    Tips:\n",
        "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
        "      In fact, the number of examples during test/train is different.\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (approx. 2 lines)\n",
        "    X = tf.placeholder(tf.float32, [n_x, None], name=\"X\")\n",
        "    Y = tf.placeholder(tf.float32, [n_y, None], name=\"Y\")\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-je5lq07wqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, Y = create_placeholders(12288, 6)\n",
        "print(\"X = \" + str(X))\n",
        "print(\"Y = \" + str(Y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRsMOoMO7wvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: initialize_parameters\n",
        "\n",
        "def initialize_parameters():\n",
        "    \"\"\"\n",
        "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
        "                        W1 : [25, 12288]\n",
        "                        b1 : [25, 1]\n",
        "                        W2 : [12, 25]\n",
        "                        b2 : [12, 1]\n",
        "                        W3 : [6, 12]\n",
        "                        b3 : [6, 1]\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
        "    \"\"\"\n",
        "    \n",
        "    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n",
        "        \n",
        "    ### START CODE HERE ### (approx. 6 lines of code)\n",
        "    W1 = tf.get_variable(\"W1\", [25, 12288], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
        "    b1 = tf.get_variable(\"b1\", [25, 1], initializer = tf.zeros_initializer())\n",
        "    W2 = tf.get_variable(\"W2\", [12, 25], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
        "    b2 = tf.get_variable(\"b2\", [12, 1], initializer = tf.zeros_initializer())\n",
        "    W3 = tf.get_variable(\"W3\", [6, 12], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
        "    b3 = tf.get_variable(\"b3\", [6, 1], initializer = tf.zeros_initializer())\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2,\n",
        "                  \"W3\": W3,\n",
        "                  \"b3\": b3}\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bMy4kmi7wyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "with tf.Session() as sess:\n",
        "    parameters = initialize_parameters()\n",
        "    print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "    print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "    print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "    print(\"b2 = \" + str(parameters[\"b2\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi6FarY47wuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: forward_propagation\n",
        "\n",
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
        "                  the shapes are given in initialize_parameters\n",
        "\n",
        "    Returns:\n",
        "    Z3 -- the output of the last LINEAR unit\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve the parameters from the dictionary \"parameters\" \n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    W3 = parameters['W3']\n",
        "    b3 = parameters['b3']\n",
        "    \n",
        "    ### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\n",
        "    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n",
        "    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n",
        "    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n",
        "    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n",
        "    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return Z3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ameY5bA87wNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    X, Y = create_placeholders(12288, 6)\n",
        "    parameters = initialize_parameters()\n",
        "    Z3 = forward_propagation(X, parameters)\n",
        "    print(\"Z3 = \" + str(Z3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRQcm1yf7wL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: compute_cost \n",
        "\n",
        "def compute_cost(Z3, Y):\n",
        "    \"\"\"\n",
        "    Computes the cost\n",
        "    \n",
        "    Arguments:\n",
        "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
        "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
        "    \n",
        "    Returns:\n",
        "    cost - Tensor of the cost function\n",
        "    \"\"\"\n",
        "    \n",
        "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
        "    logits = tf.transpose(Z3)\n",
        "    labels = tf.transpose(Y)\n",
        "    \n",
        "    ### START CODE HERE ### (1 line of code)\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx6EOE6l7wKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    X, Y = create_placeholders(12288, 6)\n",
        "    parameters = initialize_parameters()\n",
        "    Z3 = forward_propagation(X, parameters)\n",
        "    cost = compute_cost(Z3, Y)\n",
        "    print(\"cost = \" + str(cost))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fry9FpPT7wHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
        "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
        "    \"\"\"\n",
        "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
        "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
        "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
        "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
        "    learning_rate -- learning rate of the optimization\n",
        "    num_epochs -- number of epochs of the optimization loop\n",
        "    minibatch_size -- size of a minibatch\n",
        "    print_cost -- True to print the cost every 100 epochs\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "    \n",
        "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
        "    tf.set_random_seed(1)                             # to keep consistent results\n",
        "    seed = 3                                          # to keep consistent results\n",
        "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
        "    n_y = Y_train.shape[0]                            # n_y : output size\n",
        "    costs = []                                        # To keep track of the cost\n",
        "    \n",
        "    # Create Placeholders of shape (n_x, n_y)\n",
        "    ### START CODE HERE ### (1 line)\n",
        "    X, Y = create_placeholders(n_x, n_y)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Initialize parameters\n",
        "    ### START CODE HERE ### (1 line)\n",
        "    parameters = initialize_parameters()\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
        "    ### START CODE HERE ### (1 line)\n",
        "    Z3 = forward_propagation(X, parameters)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Cost function: Add cost function to tensorflow graph\n",
        "    ### START CODE HERE ### (1 line)\n",
        "    cost = compute_cost(Z3, Y)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
        "    ### START CODE HERE ### (1 line)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Initialize all the variables\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    # Start the session to compute the tensorflow graph\n",
        "    with tf.Session() as sess:\n",
        "        \n",
        "        # Run the initialization\n",
        "        sess.run(init)\n",
        "        \n",
        "        # Do the training loop\n",
        "        for epoch in range(num_epochs):\n",
        "\n",
        "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
        "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
        "            seed = seed + 1\n",
        "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
        "\n",
        "            for minibatch in minibatches:\n",
        "\n",
        "                # Select a minibatch\n",
        "                (minibatch_X, minibatch_Y) = minibatch\n",
        "                \n",
        "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
        "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
        "                ### START CODE HERE ### (1 line)\n",
        "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
        "                ### END CODE HERE ###\n",
        "                \n",
        "                epoch_cost += minibatch_cost / num_minibatches\n",
        "\n",
        "            # Print the cost every epoch\n",
        "            if print_cost == True and epoch % 100 == 0:\n",
        "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
        "            if print_cost == True and epoch % 5 == 0:\n",
        "                costs.append(epoch_cost)\n",
        "                \n",
        "        # plot the cost\n",
        "        plt.plot(np.squeeze(costs))\n",
        "        plt.ylabel('cost')\n",
        "        plt.xlabel('iterations (per tens)')\n",
        "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "        plt.show()\n",
        "\n",
        "        # lets save the parameters in a variable\n",
        "        parameters = sess.run(parameters)\n",
        "        print(\"Parameters have been trained!\")\n",
        "\n",
        "        # Calculate the correct predictions\n",
        "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
        "\n",
        "        # Calculate accuracy on the test set\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "\n",
        "        print(\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
        "        print(\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
        "        \n",
        "        return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUjnMH3H7wFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = model(X_train, Y_train, X_test, Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcUcE_W77wDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "\n",
        "## START CODE HERE ## (PUT YOUR IMAGE NAME) \n",
        "my_image = \"thumbs_up.jpg\"\n",
        "## END CODE HERE ##\n",
        "\n",
        "# We preprocess your image to fit your algorithm.\n",
        "fname = \"images/\" + my_image\n",
        "image = np.array(ndimage.imread(fname, flatten=False))\n",
        "my_image = scipy.misc.imresize(image, size=(64, 64)).reshape((1, 64 * 64 * 3)).T\n",
        "my_image_prediction = predict(my_image, parameters)\n",
        "\n",
        "plt.imshow(image)\n",
        "print(\"Your algorithm predicts: y = \" + str(np.squeeze(my_image_prediction)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyC0ZPwz7v81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVW_dviy7GnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}